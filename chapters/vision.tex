\chapter{Vision}\label{chap:vision}

Vision is one of the most information-rich sensor systems both humans and robots have available. However, efficiently and accurately processing the wealth of information that is generated by vision sensors is still a key challenge in the field. The goals of this chapter are to:
\begin{itemize}
\item introduce the concept of images as two-dimensional signals;
\item provide an intuition of the wealth of information hidden in low-level information;
\item introduce basic convolution and threshold-based image processing algorithms.
\end{itemize}

\section{Images as two-dimensional signals}

Images are captured by cameras containing matrices of charge-coupled devices (CCD) or similar semi-conductors (e.g. complementary metalâ€“oxide semiconductor, CMOS) that can turn photons into electrical signals. These matrices can be read out pixel by pixel and turned into digital values, for example an array of 640 by 480 three-byte tuples corresponding to the red, green, and blue (RGB) components the camera has seen. An example of such data is shown in \cref{fig:iss_closeup}; for simplicity, we show only one color channel.
Looking at the data in the matrix clearly reveals the white tile within the black tiles at the lower-right corner of the chessboard. Higher values correspond to brighter colors (white) and lower values to darker colors. We also observe that although the tiles have to have the same color, the actual values differ quite a bit. It might make sense to think about these values much like we would do if the data would be 1D signal: taking the ``derivative'', e.g., along the horizontal rows, would indicate areas of big changes, whereas a ``frequency'' histogram of an image  would indicate how quickly values change. Areas with smooth gradients, e.g., black and white tiles, would then have low frequencies, whereas areas with strong gradients, would contain high frequency information.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/iss_closeupmatrix}
    \caption{A chessboard floating inside the ISS with astronaut Gregory Chamitoff. The inset shows a sample of the actual data recorded by the image sensor. One can clearly recognize the contours of the white tile.}
    \label{fig:iss_closeup}
\end{figure}

This language opens the door to a series of signal processing concepts, such as low-pass filters (suppressing high frequency information), high-pass filters (suppressing low frequency information), or band-pass filters (letting only a range of frequencies pass), analysis of the frequency spectrum of the image (the distribution of content at different frequencies), or ``convolving'' the image with another two-dimensional function. The next sections will provide both an intuition of what kind of meaningful information is hidden in such abstract data and provide concrete examples of signal processing techniques that make this information appear.

\section{From signals to information}

Unfortunately, many phenomena that often have very different or even opposite meaning look very similar when looking at the low-level signal. For example, drastic changes in color values do not necessarily mean that the color of a surface indeed has changed. Similar patterns are generated by depth discontinuities, specular highlights, changing lighting conditions, or surface orientation changes. These phenomena---some of which are illustrated in \cref{fig:iss_edges}---make computer vision a hard problem.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figs/iss_edges}
    \caption{Inside of the international space station (left), circled areas in which pixel values change drastically (right). Underlying effects that produce similar responses: change in surface properties (1), depth discontinuities (2), specular highlights (3), changing lighting conditions such as shadows (4), or surface orientation changes (5).
    \label{fig:iss_edges}}
\end{figure}

This example illustrates that signals and data alone are not sufficient to understand a phenomenon, but require context. Here, the context does not only refer to surrounding signals, but also high-level conceptional knowledge such as the fact that light sources create shadows and specular highlights, that objects in the front appear larger, and so on. The importance of such conceptional knowledge is illustrated in \cref{fig:craters}:
both images show an identical landscape that once appears to be speckled with craters, once with bubble-like hills. At first glance, both scenes are illuminated from the left, suggesting a change in the landscape. However, once information that the sun is illuminating one picture from the left and the other from the right is available, the paradox becomes clear: the variable illumination makes the craters look like bumps under come conditions.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{figs/craters}
    \caption{Picture of the Apollo 15 landing site during different times of the day. The landscape is identical, but appears to be either speckled with craters (left) or hills (right). Knowing that the sun is illuminating the scene from the left and right, respectively, does explain this effect. Image credit: NASA/GSFC/Arizona State University.
    \label{fig:craters}}
\end{figure}

More surprisingly, conceptual knowledge is often sufficient to make up for the lack of low-level cues in an image. An example is shown in \cref{fig:dalmatian}. Here, a Dalmatian dog can be clearly recognized despite the absence of cues for its outline, i.e.\ by simply extrapolating its appearance and pose from conceptual knowledge.

These examples illustrate both the advantages and drawbacks of a signal processing approach to computer vision. While an algorithm will detect interesting signals even there where we don't see or expect them (due to conceptional bias), image understanding not only requires low-level processing, but also intelligent combination of the spatial relationship between low-level cues and conceptual knowledge about the world. As we will later see (\cref{chap:ann}), this can be accomplished through convolutional neural networks that provide a single pipeline to process information at different scales---ranging from extracting local features to examining their spatial relationships with each other.

\begin{figure}[!]
    \centering
    \includegraphics[width=\textwidth]{figs/dalmatian}
    \caption{The image of a Dalmatian dog can be clearly recognized by most spectators even though low-level cues such as edges are only present for ears, chin and parts of the legs. The contours of the animals are highlighted in a flipped version of the image in the inset.
    \label{fig:dalmatian}}
\end{figure}

\section{Basic image operations}

Basic image operations can be thought of as a filter that operates in the frequency or in the space (intensity/color) domain. Although most filters directly operate in the intensity domain, knowing how they affect the frequency domain is helpful in understanding the filter's function. For example, a filter that is supposed to highlight edges such as the one shown in \cref{fig:iss_edges} should suppress low frequencies---i.e., areas in which the color values do not change much, and amplify high-frequency information---i.e., areas in which the color values change quickly. The goal of this section is to provide a basic understanding of how basic image processing operation works. It is important to note that the methods presented here, while still valid, have been superseded by more sophisticated implementations that are widely available as software packages or within desktop graphic software.

\subsection{Threshold-based operations}
In order to find objects with a certain color or edge amplitude, thresholding an image will lead to a binary image that contains ``true-false'' regions that fit the desired criteria. Thresholds make use of operators like $>,<,\leq,\geq$ and combinations thereof. There also exist adaptive versions that adapt/update the thresholds locally, e.g., to make up for changing lighting conditions.
Albeit thresholding is simple if compared to other techniques shown below, finding correct threshold values is a hard problem. In particular, actual pixel values change drastically with change in the lighting conditions and there is no such thing as ``red'' or ``green'' when inspecting the actual values under different conditions.

\subsection{Convolution-based filters}

\screencast{https://commons.wikimedia.org/wiki/File:Convolution_of_box_signal_with_itself2.gif}{convolution}

A filter can be implemented using the \textsl{convolution}\index{Convolution} operator $\star$ that convolves function $f()$ with function $g()$:
\begin{equation}
f(x)\star g(x)=\int_{-\infty}^{\infty}f(\tau)g(x-\tau)d\tau \quad,
\end{equation}
where $g()$ is defined as \textsl{filter}\index{Filter}.
The convolution essentially ``shifts'' the function $g()$ across the function $f()$ while multiplying the two (see also in the video to the left). As images are discrete signals, the convolution is consequently discrete:
\begin{equation}
f[x]\star g[x]=\sum_{i=-\infty}^{\infty}f[i]g[x-i]\quad .
\end{equation}
Additionally, given that images are two-dimensional signals, the convolution is two-dimensional as well:
\begin{equation}\label{eq:2dconv1}
f[x,y]\star g[x,y]=\sum_{i=-\infty}^{\infty}\sum_{j=-\infty}^{\infty}f[i,j]g[x-i,y-j]\quad.
\end{equation}
Although we have defined the convolution from negative infinity to infinity, both images and filters are usually finite. Images are constrained by their resolution, and filters are usually much smaller than the images themselves. Also, the convolution is commutative, therefore \cref{eq:2dconv1} is equivalent to:
\begin{equation}\label{eq:2dconv2}
f[x,y]\star g[x,y]=\sum_{i=-\infty}^{\infty}\sum_{j=-\infty}^{\infty}f[x-i,y-j]g[i,j].
\end{equation}

\subsubsection{Gaussian smoothing}

One of the most basic (and important) filters is the \textsl{Gaussian filter}\index{Gaussian filter}. The Gaussian filter is shaped like the Gaussian bell function and can be easily stored in a two-dimensional matrix. Implementing a Gaussian filter is surprisingly simple, e.g.:
\begin{equation}
g(x,y)=\frac{1}{10}\quad.
\left(
\begin{array}{ccc}
1 & 1 & 1\\
1 & 2 & 1\\
1 & 1 & 1\\
\end{array}
\right)
\end{equation}
Using this filter in Equation~\ref{eq:2dconv2} on an infinitely large image $f()$ leads to
\begin{equation}\label{eq:2dconv3}
f[x,y]\star g[x,y]=\sum_{i=-1}^{1}\sum_{j=-1}^{1}f[x-i,y-j]g[i,j]\quad,
\end{equation}
assuming that $g(0,0)$ is the center of the matrix. What now happens is that each pixel $f(x,y)$ becomes the average of that of its neighbors, with its previous value weighted twice than that of its neighbors (because $g(0,0)=0.2$). More concretely:
\begin{equation}
f(x,y)=
\begin{smallmatrix*}[l]
f(x+1,y+1)g(-1,-1) &+f(x+1,y)g(-1,0) &+f(x+1,y-1)g(-1,1)\\
+f(x,y+1)g(0,-1) &+f(x,y)g(0,0) &+f(x,y-1)g(0,1)\\
+f(x-1,y+1)g(1,-1) &+f(x-1,y)g(1,0) &+f(x-1,y-1)g(1,1)
\end{smallmatrix*}
\end{equation}
Doing this for all $x$ and all $y$ is equivalent to physically ``sliding'' the filter $g()$ along the image.

\begin{figure}[!]
    \centering
    \includegraphics[width=\textwidth]{figs/filters}
    \caption{A noisy image before (top left) and after filtering with a Gaussian kernel (top right). Corresponding edge images are shown underneath.
    \label{fig:filters}}
\end{figure}

An example of the Gaussian filter in action is shown in \cref{fig:filters}. The filter acts as a \textsl{low-pass filter}\index{Low-pass filter}, suppressing high frequency components. Indeed, noise in the image is suppressed, leading also to a smoother edge image, which is shown to the right.

\subsubsection{Edge detection}\label{sec:sobel}

Edge detection can be achieved using another convolution-based filter, the \textsl{Sobel} kernel\index{Sobel filter}:
\begin{equation}
s_x(x,y)=
\left(
\begin{array}{ccc}
-1 & 0 & 1\\
-2 & 0 & 2\\
-1 & 0 & 1\\
\end{array}
\right)
\qquad
s_y(x,y)=
\left(
\begin{array}{ccc}
1 & 2 & 1\\
0 & 0 & 0\\
-1 & -2 & -1\\
\end{array}
\right)
\end{equation}
Here, $s_x(x,y)$ can be used to detect vertical edges, whereas $s_y(x,y)$ highlights horizontal edges. Edge detectors such as the \textsl{Canny} edge detector\index{Canny edge detector} therefore run at least two of such filters over an image to detect both horizontal and vertical edges.

\subsubsection{Difference of Gaussians}
\label{sec:vision:dog}

An alternative method for detecting edges is the \textsl{Difference of Gaussians} (DoG) method\index{Difference of Gaussians (DoG)}. The idea is to subtract two images that have each been filtered using a Gaussian kernel with different width. Both filters supress high-frequency information and their difference therefore leads to a \textsl{band-pass} filtered signal\index{Band-pass filter}, from which both low and high frequencies have been removed. As such, a DoG filter acts as a capable edge detection algorithm. Here, one kernel is usually four to five times wider than the other, therefore acting as a much stronger filter.

Differences of Gaussians can also be used to approximate the \textsl{Laplacian of Gaussian}\index{Laplacian of Gaussian}, i.e., the sum of the second derivatives of a Gaussian kernel. Here, one kernel is roughly 1.6 times wider than the other. The band-pass characteristic of DoG and LoGs are important as they highlight high-frequency information such as edges, yet suppress high-frequency noise in the image.

\subsection{Morphological Operations}

Another class of filters are morphological operators which consist of a kernel describing the structure of the operation (this can be as simple as an identity matrix) and a rule on how to change a pixel value based on the values in the neighborhood defined by the kernel.
%
Important morphological operators are \textsl{erosion} and \textsl{dilation}\index{Erosion}\index{Dilation}. The erosion operator assigns a pixel a value with the minimum value that it can find in the neighborhood defined by the kernel. The dilation operator assigns a pixel a value with the maximum value it can find in the neighborhood defined by the kernel. This is useful, e.g., to fill holes in a line or remove noise. A dilation followed by an erosion is known as a ``Closing'' and an erosion followed by a dilation as an ``Opening''. Subtracting erosed and dilated images from each other can also serve as an edge detector. Examples of such operators are shown in \cref{fig:morphology}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/morphology}
    \caption{Examples of morphological operators erosion and dilation and combinations thereof (image credit: OpenCV documentation, BSD).
    \label{fig:morphology}}
\end{figure}

\section{Extracting Structure from Vision}\label{sec:vision:structure}

A remarkable property of vision is the ability to provide both semantic (\textsl{qualities} of the scene, such as what is in it) and metric (\textsl{quantities} of the scene, such as sizes and distances) information. Extracting semantic information is nowadays heavily reliant on machine learning, which is explained at a high level in \cref{sec:cvml}. The extraction of metric information however can be accomplished by leveraging geometric relationships, which we will describe here.

\cref{fig:stereovision} shows a high-level schematic of the relationships between an image frame and another---both observing the same point. In here, we do not draw a distinction between these two frames being either spatially or temporally correlated, which are two distinct problems in robotics: in \textsl{stereo vision}\index{Stereo Vision}, two cameras are rigidly attached to one another (spatial correlation) and are acquiring images of the same scene; in \textsl{structure from motion}\index{Structure From Motion}, a single camera is moved through a scene and a pair of images from the single camera are related to one another via a transform matrix (temporal correlation).
In either case, it is possible to identify the ``projection center'' of the camera frames as $C_L$ and $C_R$; they related to one another through $T_{LR}$, which is defined as the transform matrix from the left to the right frame. In stereo vision, this transform is known as the \textsl{sensor extrinsics}, a 6-dof quantity that must be estimated through calibration. In structure from motion, this transform quantifies the motion applied to the camera, which can be estimated through localization (see \cref{chap:localization}).

\begin{figure}
\centering
    \def\svgwidth{1.0\textwidth} 
    \import{./figs/}{stereovision.pdf_tex}
\caption{Schematic of correlating features across images in order to extract three-dimensional information from two-dimensional views.}
\label{fig:stereovision}
\end{figure}

Note that since the camera pair takes two images of the same scene, two projections into the corresponding image planes of the same point in the world can be correlated with one another to determine the point's 3D position. This may be accomplished naively through identifying the point $p_{P_L}$ in $C_L$ and searching for that point $p_{P_R}$ in $C_R$. Crucially, the nature of projections of 3D points into the camera frame is a known operation, and in fact a very simple one. In particular, a 3D point in the world can be projected into the camera frame using:
\screencast{https://youtu.be/ND2fa08vxkY}{cameraparameters}
\begin{equation}
\begin{pmatrix}
u\\
v\\
1
\end{pmatrix}
= K T \begin{pmatrix}
x\\
y\\
z\\
1 
\end{pmatrix}
\label{eq:stereovision}
\end{equation}

\noindent where $K$ is known as the \textsl{camera intrinsic matrix} and $T$ is the matrix form of the transform between the camera and some global coordinates in which the point $(x, y, z)$ is expressed. Note that the \textsl{camera intrinsic matrix} is another $\textsl{calibrated}$ quantity, instantiated by two optical center parameters and two scaling parameters.
Importantly, it is possible for two projected points (representing a single point in 3D space) to be calculated directly through triangulation on these 2D point pairs in images alone. Using the same math as in Eq.\ \eqref{eq:stereovision} but expressing $C_L$ as the global coordinate system, we can relate the 3D coordinate of the point to the two 2D measurements, camera intrinsic matrix, and $T_{LR}$:

\begin{equation}
\begin{pmatrix}
u_R\\
v_R\\
1
\end{pmatrix}
= K T_{LR} \begin{pmatrix} x \\ y \\ z \\ 1\end{pmatrix}
= K T_{LR} K^{-1} \begin{pmatrix} u_L \\ v_L \\ 1 \end{pmatrix}
\end{equation}

Note that this expression is frequently given in terms of what is known as the ``essential matrix,'' which is nominally a technique to solve this problem for uncalibrated cameras. This expression, clearly induced by the geometry expressed in \cref{fig:stereovision}, allows for alternatively solving of the values making up the entries of the essential matrix and not those of the camera intrinsic and extrinsic parameters.

Looking back at the geometry in \cref{fig:stereovision}, it may be noted that $p_{P_L}$ lies on a line that extends from the center of camera $C_L$ to the point $p$. However, there is ambiguity of the depth along ray that is cast from $C_L$ to $p_c$ $\overrightarrow{C_L p_c}$. In order to disambiguate this depth, the line between $p_{P_L}$ and the center of projection of $P_L$, which is known as the ``epipolar line,'' \index{Epipolar Line} may be projected into $P_R$, which creates the so-called ``epipolar line of $p_L$ in $C_R$.'' It is along this line that the point $p_c$ projected into $P_R$ may be found. Most notably from this is that therefore the search for the point $p_{P_R}$ may be reduced to a line search along the projected epipolar line. This line search is much more efficient than finding the projection of point $p_c$ in the whole image plane of $P_R$, and therefore allows for rapidly sped-up geometry calculations across image pairs.

%There is no single way to approach the problem of metric information extraction from images, but these techniques should provide some insight into the general manipulations executed in order to extract these data. With a little geometry, there is clearly much more than meets the eye in images than objects and classes.

%We will begin by explaining the motivation behind image coordinates. We note in \cref{fig:imagecoordinates} that in a perfect ``linear'' camera, a particular pixel in an image is given coordinates $(u,v)$ from the center point of the image. That is, the center point is denoted the origin $(0,0)$; this differs from standard image encodings, where the ``top left'' of the image is the origin. Frequently this requires $(c_x, c_y)$.

Extracting metric information from images requires to uniquely identifying identical points in each image. A simple solution to this problem is what is known as \textsl{structured light}\index{Structured light} and is illustrated in \cref{fig:struclight}.
%
Thanks to the continuously increasing efficiency of computational systems, a light-weight version of such an approach has become feasible to be implemented at small scale and low cost around $2010$, and emerged as a novel standard in robotic sensing.

\begin{figure}
	\centering
		\includegraphics[width=\textwidth]{figs/structuredlight.png}
	\caption{From left to right: two complex physical objects, a pattern of colored straight lines and their deformation when hitting the surfaces, reconstructed 3D shape. From \protect\cite{zhang2002rapid}.}
	\label{fig:struclight}
\end{figure}

Instead of using line patterns, infrared-based depth image sensors use a speckle pattern (a collection of randomly distributed dots with varying distances).
%, and two computer vision concepts: \textsl{depth from focus} and \textsl{depth from stereo}.\index{Depth from Focus}\index{Depth from Stereo} When using a lens with a narrow focal depth, objects that are closer or farther away appear blurred; you can easily observe this on professional portrait photos, which often use this effect for aesthetic purposes under the name of ``bokeh''.
%Measuring the ``blurriness'' of a scene (for known camera parameters) therefore allows an initial estimate of depth.
%Conversely, depth from stereo works by measuring the disparity of the same object appearing in two images taken by cameras that are a known distance apart. Being able to identify the same object in both frames allows to calculate this disparity, and from there the distance of the object: the farther the object is, the smaller the disparity will be.
Identifying identical points in two images simply requires to search for blobs with similar size that are close to each other.


\section{Computer Vision and Machine Learning}
\label{sec:cvml}
The algorithms described here still form the basis of most image understanding pipelines and make feature detection (\cref{chap:feature_extraction}) tractable. With the advent of so-called ``convolutional neural networks'' (\cref{chap:ann}), basic signal processing such as described here is now often wrapped into the image understanding problem. While this makes it less important to implement such algorithms oneself, understanding what convolution, morphological operations and thresholds do to visual information remains still relevant to meaningfully compose neural networks and make them less of a black box.


\section*{Take-home lessons}
\begin{enumerate}
\item Unlike the sensors from \cref{chap:sensors}, our brains can directly process the 2D information that is captured by a vision sensor. It is difficult to unthink the amount of processing that we perform automatically, augmenting the signal with knowledge and other information that the computer does not necessarily have.
\item  Algorithms described in this chapter aim at reducing information to a lower-dimensional space by removing noise and other spurious information, making the related challenge of understanding the data more tractable. 
\item There is a trade-off between making the data stream more tractable and preserving actual information. As computers and algorithms, in particular machine learning, become more powerful, modern vision systems often blend pre-processing and actual image understanding into a single pipeline. 
\end{enumerate}


\section*{Exercises}\small
\begin{enumerate}
\item Below are shown multiple ``Kernels'' that can be used for convolution-based image filtering.
\begin{equation}
\nonumber
\begin{array}{|c|c|c|}
\hline
1 & 1 & 1\\
\hline
1 & 2 & 1\\
\hline
1 & 1 & 1\\
\hline
\end{array}
\quad
\begin{array}{|c|c|c|}
\hline
0 & -1 & 0\\
\hline
0 & -1 & 0\\
\hline
0 & -1 & 0\\
\hline
\end{array}
\quad
\begin{array}{|c|c|c|}
\hline
1 & 1 & 1\\
\hline
1 & -4 & 1\\
\hline
1 & 1 & 1\\
\hline
\end{array}
\end{equation}
\begin{enumerate}
\item Identify the Kernel, which can blur an image.
\item What kind of features can be detected by the other two kernels?
\end{enumerate}
\item How many for-loops are needed to implement a 2D convolution? Explain your reasoning.
\item Use an appropriate robot simulation environment that allows you access to a simulated camera in a world with simple features such as geometric shapes of different color etc.
\begin{enumerate}
\item Implement a thresholding algorithm that allows you to black out anything but an object of a specific color. Is a simple threshold enough? Why not? Can you black out an object using a low and and a high threshold?
\item Implement a smoothening algorithm by performing both a convolution with a Gaussian kernel as well as a series of morphological operations. Experiment with kernels of different width and different steepness. What are the advantages and drawbacks of using morphological operations over a simple Gaussian filter?
\item Implement an edge detection algorithm, e.g. by performing a convolution with a Sobel kernel. Experiment with different kernels. What else do you need to do to create an image that only contains edges?
\end{enumerate}
\item Can you think about a smoothening algorithm that will only smoothen small amounts of noise, but maintains edges? What kind of filtering algorithms could you combine to achieve this goal?
\item Explore the internet for a computer vision toolbox that supports your language of choice. What do you find? Does the toolbox implement all of the algorithms in this chapter? Solve the above assignments using the toolbox's built-in functions.
\item Use an appropriate robot simulation environment that allows you to simulate two cameras that are at a known distance in the same plane. Use simple geometric objects such as a red ball and compute their distance using stereo disparity. 
\end{enumerate} \normalsize
